{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c5c044f",
   "metadata": {},
   "source": [
    "Great! Let‚Äôs take the **logic of repeated experimentation + probability** (as you mentioned) and use it to **explain evaluation metrics in machine learning**.\n",
    "\n",
    "We‚Äôll look at how these metrics help us understand whether our ‚Äúrepeated experiments‚Äù (predictions) are **working well**‚Äîand how **probability** plays a central role.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† The Logic: ML = Repeated Experiments + Probability\n",
    "\n",
    "In machine learning:\n",
    "\n",
    "* You **try different models or parameters** (like repeated experiments).\n",
    "* The model makes **probabilistic predictions** (e.g., ‚Äúthere‚Äôs a 78% chance this person will be employed‚Äù).\n",
    "* You **evaluate** those predictions to see:\n",
    "  **‚ÄúHow good are they, and how can I improve them?‚Äù**\n",
    "\n",
    "Evaluation metrics are the **feedback** we get from each \"experiment\" (prediction).\n",
    "They help us **measure success**, adjust parameters, and **optimize the model**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Main Evaluation Metrics Explained with This Logic\n",
    "\n",
    "Let‚Äôs break them down into two main categories: **Classification** and **Regression**.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Classification Metrics\n",
    "\n",
    "(Used when the output is a category, like \"yes\"/\"no\", \"spam\"/\"not spam\")\n",
    "\n",
    "### 1. **Accuracy**\n",
    "\n",
    "> **What % of predictions were correct?**\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}\n",
    "$$\n",
    "\n",
    "* Logic: You're testing a hypothesis multiple times (each prediction).\n",
    "* You‚Äôre just counting how many times the experiment gave the **right result**.\n",
    "\n",
    "üîÅ **Works well** when classes are balanced.\n",
    "‚ùó **Can be misleading** if data is imbalanced (e.g., 90% of patients are healthy).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Precision**\n",
    "\n",
    "> Of all the times the model **predicted positive**, how many were actually correct?\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}\n",
    "$$\n",
    "\n",
    "* Logic: Imagine an experiment where your model **declares something is true** (e.g., someone will get a job).\n",
    "* Precision tells you: **‚ÄúHow often is that claim right?‚Äù**\n",
    "\n",
    "Useful when **false positives are costly**, like in spam detection or medical diagnosis.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Recall (Sensitivity)**\n",
    "\n",
    "> Of all the actual positives, how many did the model find?\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\n",
    "$$\n",
    "\n",
    "* Logic: You want to catch all positive cases.\n",
    "* It's like repeating an experiment to **find all true cases**, and seeing **how many you missed**.\n",
    "\n",
    "Useful in health, crime detection, etc., where **missing a real positive is dangerous**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **F1 Score**\n",
    "\n",
    "> Harmonic mean of precision and recall:\n",
    "\n",
    "$$\n",
    "\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision + Recall}}\n",
    "$$\n",
    "\n",
    "* Logic: A balance between **being precise** and **not missing real positives**.\n",
    "* Great when you need to **optimize both goals** in a noisy environment.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **AUC-ROC Curve**\n",
    "\n",
    "> Measures how well the model separates classes across **all thresholds**.\n",
    "\n",
    "* You test how the model behaves at **different levels of certainty**.\n",
    "* Think of it as: \"How well can my probabilistic model **rank** the true positives higher than false ones?\"\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Regression Metrics\n",
    "\n",
    "(Used when output is a continuous value, like price, age, income)\n",
    "\n",
    "### 1. **Mean Absolute Error (MAE)**\n",
    "\n",
    "> Average of absolute differences between prediction and true value.\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "* Logic: Run many experiments, calculate how far off you are each time.\n",
    "* MAE is like saying: \"On average, how much is my prediction **wrong by**?\"\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Mean Squared Error (MSE) / Root MSE (RMSE)**\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\text{MSE}}\n",
    "$$\n",
    "\n",
    "* Squaring exaggerates **larger errors**, so these metrics **penalize big mistakes** more.\n",
    "* Logic: If some experiments fail **badly**, these metrics show that very clearly.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **R-squared (R¬≤ Score )**\n",
    "\n",
    "> What proportion of variance in the data is explained by the model?\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\text{Sum of Squared Errors}}{\\text{Total Variance}}\n",
    "$$\n",
    "\n",
    "* Logic: You're comparing the model‚Äôs prediction error to a ‚Äúdumb‚Äù prediction (mean).\n",
    "* R¬≤ tells you: ‚ÄúHow much better is my experiment than random guessing?‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Summary Table (Logic View)\n",
    "\n",
    "| Metric     | Logic Interpretation                        | When to Use                                    |\n",
    "| ---------- | ------------------------------------------- | ---------------------------------------------- |\n",
    "| Accuracy   | How many experiments got the right outcome  | Balanced classification problems               |\n",
    "| Precision  | Of predicted positives, how many were right | When false positives are costly                |\n",
    "| Recall     | Of real positives, how many were found      | When false negatives are costly                |\n",
    "| F1 Score   | Balance of precision and recall             | When both errors matter                        |\n",
    "| MAE        | Avg. size of experimental error             | Easy to interpret, no exaggeration of outliers |\n",
    "| MSE / RMSE | Penalizes large experimental errors heavily | When large mistakes are especially bad         |\n",
    "| R-squared  | How well does the model improve over chance | Regression; model quality in total context     |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a visual explanation or interactive code to experiment with these metrics in Python?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d142351",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobwatcher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
